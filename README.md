# Advanced-Predictive-Modeling
  
In this UT Austin course, I went over more of the nuances and theory behind various machine learning models. I worked with Chirag Ramesh on these assignments and have listed my and his contribution in each of the assignment folders. Our code is provided but data files are not. A brief overview of the assignment details and my contributions are listed here:
  
## Assignment 1:
This assignment involved testing our knowledge of Maximum Likelihood Estimation, Multi-Linear Regression, Lasso regression, and Ridge regression. For this assignment, I showcase my knowledge for Maximum Likelihood Estimation and served as a consultant for analyzing the results of the regression models. 
  
## Assignment 2:  
This assignment involved testing our knowledge of lasso regression and ridge regression again to compare it with the ElasticNet method. Additionally, the assignment explored the bias variance tradeoff by doing non linear model regressions and tasked us with deriving the gradients of a nonlinear model for Stochastic Gradient Descent as well as implementing it using a Teacher Assistant provided incomplete class shell. I handled the latter two parts.  
  
## Assignment 3:
This assignment tasked us with exploring neural networks, feature selection, and data preprocessing methods. As Chirag was busy at the time, I handled all of the code for this assignment. Analysis wise, I analyzed pre processing and assisted Chirag with his analysis for multi-layer perception neural networks and feature selection. We ended up going with my analysis when playing around with neural network parameters in google's tensor playground.  
  
## Assignment 4:  
For this assignment, we did principal component analysis, decision tree classification, sklearn pipeline, and explored rejection ranges. I handled the decision tree and rejection range part. 
  
## Assignment 5:  
Our last assignment involved bayesian belief networks, random forests, regularized logistic regression vs decision trees, comparing different ensemble methods, and had a bonus question involving shapely values. For this assignment, I worked on the ensemble method comparison, polished the formatting for bayesian belief networks, and did part of the bonus question. As I was time constrained at the time, I manged to answer the part about shapely values but not implement them into a decision tree.